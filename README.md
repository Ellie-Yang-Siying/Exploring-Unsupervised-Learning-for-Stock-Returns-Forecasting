# Exploring Unsupervised Learning for Stock Returns Forecasting
Refer to the paper here: [Project_Report.pdf](https://github.com/user-attachments/files/19061671/Project_Report.pdf)



## Introduction
Financial markets are complex systems influenced by numerous interconnected factors and non-linear dynamics. Predicting stock prices requires methods that uncover patterns without overfitting to noise. Traditional models like ARIMA are limited by linear assumptions, while advanced techniques such as Long Short-Term Memory networks (LSTMs) can model non-linear dependencies. In this project, we explore the hypothesis that clustering stocks by time series similarity enhances the accuracy of the forecasting models. Using k-means clustering with Dynamic Time Warping (DTW) and Fourier Coefficients as distance metrics, we group FTSE 250 stocks to isolate meaningful patterns within clusters. These will serve as the foundation for both linear and non-linear predictive models. Specifically, we train ARIMA, LightGBM, and LSTM models on clustered data, comparing their performance against models trained on individual stock series or the entire dataset. We aim to determine whether clustering helps filter out irrelevant signals while incorporating valuable contextual data, addressing overfitting and noise.

## Data
For this project, we used historical stock price data from the FTSE 250 index, sourced from Yahoo Finance. The dataset includes daily trading information, such as open, high, low, close, adjusted close prices, and trading volumes over the past 10 years. Our analysis focuses on the adjusted close price, which accounts for corporate actions like dividends and splits, providing a consistent basis for modeling. The FTSE 250, a benchmark index of mid-cap stocks in the UK, offers a diverse cross section of industries, making it ideal for clustering and predictive modeling. The decade-long time span captures varied market conditions, providing a robust foundation for testing our methods. After loading and verifying the raw FTSE 250 historical stock data, we performed critical preprocessing to ensure meaningful and statistically sound analyses. We inspected the dataset for missing values, discarding tickers with over 20% gaps and using time-based interpolation and forward/backward filling for the rest, reducing the sample to 169 stocks. Anomalies, including a misreported ticker scaled by a factor of 100, were corrected. We then standardized each time series using rolling means and standard deviations to normalize price fluctuations, ensuring comparability across stocks for clustering. Finally, the cleaned dataset was formatted for clustering and predictive modeling.

## Findings
Our results indicate that using clustered data generally outperforms training on individual stocks but shows no significant improvement over training on all data. This can be attributed to the relatively small dataset size (162 stocks) and the limited noise within the dataset, which reduces the theoretical advantage of clustering. When comparing LightGBM and LSTM, we observe that the boosted tree model performs slightly better. This advantage is primarily due to LightGBMâ€™s fast execution speed, which allows it to handle more features and larger window sizes effectively. LSTM still have great potential for improvement with access to more computational resources.

## Contributors
Ellie Yang, Michelle Zhou, Steve Zhou, Ismail Berbache, Alessandro Morosini
